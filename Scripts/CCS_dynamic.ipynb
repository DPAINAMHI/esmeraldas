{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.warp import Resampling\n",
    "from rasterio.crs import CRS\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.mask import mask\n",
    "import urllib.request\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_persiann_css_online0(url, nrow, ncol, dtype=np.int16):\n",
    "    \"\"\"Downloads, decompresses a Persiann CCS .bin.gz file, converts it to a NumPy array,\n",
    "        sets -9999 values to NaN, divides all values by 100, and reshapes to 2x2.\n",
    "\n",
    "        Args:\n",
    "            url: The URL of the Persiann CCS .bin.gz file.\n",
    "            dtype: The desired data type for the NumPy array (default: np.float32).\n",
    "\n",
    "        Returns:\n",
    "            A reshaped NumPy array containing the processed data as a nrow*ncol matrix.\n",
    "\n",
    "        Raises:\n",
    "            URLError: If an error occurs while downloading the file.\n",
    "            ValueError: If the decompressed data size is not compatible with nrow*ncol reshape.\n",
    "    \"\"\"\n",
    "    # Try opening the URL and decompressing the data\n",
    "    try:\n",
    "        compressed_data = requests.get(url).content\n",
    "        decompressed_data = gzip.decompress(compressed_data)\n",
    "        # Convert to NumPy array\n",
    "        data = np.frombuffer(decompressed_data, dtype=np.dtype('>h')).astype(float) \n",
    "        data = data.reshape((nrow,ncol))\n",
    "        data_1 = data[:,int(ncol/2):]\n",
    "        data_2 = data[:,:int(ncol/2)]\n",
    "        data = np.hstack((data_1,data_2))\n",
    "        data= data/100\n",
    "        data[data < 0] = np.nan\n",
    "        data = np.flipud(data)\n",
    "        print(f\"Data successfully downloaded from {url}\")\n",
    "        compressed_data = None\n",
    "        decompressed_data = None\n",
    "        data_1 = None\n",
    "        data_2 = None\n",
    "        del compressed_data\n",
    "        del decompressed_data\n",
    "        del data_1\n",
    "        del data_2\n",
    "        return data\n",
    "    except urllib.error.URLError as e:\n",
    "        raise urllib.error.URLError(f\"Error reading file from {url}: {e}\")\n",
    "\n",
    "\n",
    "def read_persiann_css_online(url, nrow, ncol):\n",
    "    \"\"\"Downloads, decompresses a Persiann CCS .bin.gz file, converts it to a NumPy array,\n",
    "       sets -9999 values to NaN, divides all values by 100, and reshapes to 2x2.\n",
    "       With visualized progress bar while downloading each file\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the Persiann CCS .bin.gz file.\n",
    "        nrow: Number of rows for reshaping the data.\n",
    "        ncol: Number of columns for reshaping the data.\n",
    "        dtype: The desired data type for the NumPy array (default: np.float32).\n",
    "\n",
    "    Returns:\n",
    "        A reshaped NumPy array containing the processed data as a nrow*ncol matrix.\n",
    "\n",
    "    Raises:\n",
    "        URLError: If an error occurs while downloading the file.\n",
    "        ValueError: If the decompressed data size is not compatible with nrow*ncol reshape.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Open the URL, handle unknown content length\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            total_size = int(response.headers.get('content-length', None))  # Get total size (if available)\n",
    "            progress_bar = tqdm(total=total_size, desc=\"Downloading data from \"+url, unit='B', unit_scale=True, unit_divisor=1024) if total_size is not None else tqdm(desc=\"Downloading data\")\n",
    "\n",
    "            # Alternative 1: Read data in chunks using read(chunksize)\n",
    "            compressed_data = b''\n",
    "            chunksize = 1024*100 # update per 100 kb of downloading\n",
    "            while True:\n",
    "                chunk = response.read(chunksize)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                compressed_data += chunk\n",
    "                if total_size is not None:  # Update progress if total size known\n",
    "                    progress_bar.update(len(chunk))\n",
    "\n",
    "            # Alternative 2: Read entire data (if content length is known or for small files)\n",
    "            # if total_size is not None:\n",
    "            #     compressed_data = response.read()\n",
    "\n",
    "            progress_bar.close()  # Close progress bar after download\n",
    "\n",
    "        # Decompress data\n",
    "        decompressed_data = gzip.decompress(compressed_data)\n",
    "        data = np.frombuffer(decompressed_data, dtype=np.dtype('>h')).astype(float) \n",
    "        data = data.reshape((nrow,ncol))\n",
    "        data_1 = data[:,int(ncol/2):]\n",
    "        data_2 = data[:,:int(ncol/2)]\n",
    "        data = np.hstack((data_1,data_2))\n",
    "        data= data/100\n",
    "        data = np.round(data,2)\n",
    "        data[data < 0] = np.nan\n",
    "        data = np.flipud(data)\n",
    "        return data\n",
    "    \n",
    "    except urllib.error.URLError as e:\n",
    "        raise urllib.error.URLError(f\"Error reading file from {url}: {e}\")\n",
    "\n",
    "def format_number_with_zeros(number, desired_digits):\n",
    "  \"\"\"Formats a number with leading zeros to reach the desired number of digits.\n",
    "\n",
    "  Args:\n",
    "      number: The integer to format.\n",
    "      desired_digits: The desired number of digits in the output string.\n",
    "\n",
    "  Returns:\n",
    "      A string representation of the number with leading zeros if needed.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(number, int) or desired_digits <= 0:\n",
    "    raise ValueError(\"Invalid input: number must be an integer and desired_digits must be positive.\")\n",
    "\n",
    "  # Convert the number to a string\n",
    "  number_str = str(number)\n",
    "\n",
    "  # Add leading zeros if needed\n",
    "  num_leading_zeros = desired_digits - len(number_str)\n",
    "  formatted_string = \"0\" * num_leading_zeros + number_str\n",
    "\n",
    "  return formatted_string\n",
    "\n",
    "def iter_url(start_year, start_month, start_day, end_year, end_month, end_day, interval, max_num_of_obs_per_slice):\n",
    "    \"\"\"\n",
    "    Generates a set of URLs and corresponding date ranges based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        start_year: The starting year of the date range.\n",
    "        start_month: The starting month of the date range.\n",
    "        start_day: The starting day of the date range.\n",
    "        end_year: The ending year of the date range.\n",
    "        end_month: The ending month of the date range.\n",
    "        end_day: The ending day of the date range.\n",
    "        interval: The interval between URLs in hours (e.g., 3 for every 3 hours).\n",
    "        max_num_of_obs_per_slice: The maximum number of observations per data slice.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "        - urls: A list of generated URLs for the slice.\n",
    "        - date_slice: A pandas date range object representing the slice.\n",
    "        - start_datetime (pd.Timestamp): The start datetime of the slice.\n",
    "        - end_datetime (pd.Timestamp): The end datetime of the slice.\n",
    "        The number of observations in the given time range combining all slices\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the start and end time points\n",
    "    start_time = pd.Timestamp(year=start_year, month=start_month, day=start_day)\n",
    "    end_time = pd.Timestamp(year=end_year, month=end_month, day=end_day, hour=23,minute=59,second=59)\n",
    "    if start_time>end_time:\n",
    "       raise ValueError(\"start date must be no more recent than the end date inputed\")\n",
    "    # Construct the time range with the specified interval and inclusive \"left\" boundary\n",
    "    freq = str(interval) + 'h'\n",
    "    time_range = pd.date_range(start=start_time, end=end_time, freq=freq, inclusive='left')\n",
    "    ranges = []\n",
    "    url_base = \"https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/\"\n",
    "\n",
    "    # Iterate over time slices\n",
    "    num_of_slice=math.ceil(len(time_range)/max_num_of_obs_per_slice)\n",
    "    for i in range(num_of_slice):\n",
    "        slice_start_index = i * max_num_of_obs_per_slice\n",
    "        slice_end_index = min((i + 1) * max_num_of_obs_per_slice - 1, len(time_range) - 1)  # Handle cases where the last slice might be shorter\n",
    "        slice_start = time_range[slice_start_index]\n",
    "        slice_end = time_range[slice_end_index]\n",
    "        date_slice = time_range[slice_start_index:slice_end_index + 1]  # Use direct slicing for DatetimeIndex\n",
    "        urls = []\n",
    "        # Iterate over all days within the slice\n",
    "        for time_point in date_slice:\n",
    "            doy = time_point.timetuple().tm_yday  # Day of the year\n",
    "            year_2d = str(time_point.year)[-2:]  # Last two digits of the year\n",
    "            doy_formatted = format_number_with_zeros(doy, 3)\n",
    "            hh_formatted = format_number_with_zeros(time_point.hour, 2)  # Hour with leading zeros\n",
    "            # Construct the URL\n",
    "            url = url_base + str(interval) + \"hrly/\" + \"rgccs\" + freq + year_2d + doy_formatted + hh_formatted + '.bin.gz'\n",
    "            urls.append(url)\n",
    "\n",
    "        range_dict = {\n",
    "            \"urls\": urls,\n",
    "            \"date_slice\": date_slice,\n",
    "            \"start_datetime\": slice_start,\n",
    "            \"end_datetime\": slice_end,\n",
    "        }\n",
    "        ranges.append(range_dict)\n",
    "\n",
    "    print(f\"{len(time_range)} datasets from {start_time.date()} to {end_time.date()} with a frequency of {freq} will be downloaded from {url_base}\")\n",
    "    return ranges, len(time_range)\n",
    "\n",
    "def num_to_ordinal(num):\n",
    "  \"\"\"\n",
    "  Converts a number to its ordinal representation.\n",
    "\n",
    "  Args:\n",
    "      num: The number to be converted (must be an integer).\n",
    "\n",
    "  Returns:\n",
    "      str: The ordinal representation of the number (e.g., \"1st\", \"2nd\", \"3rd\", etc.).\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If the input number is not an integer.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(num, int):\n",
    "    raise ValueError(\"Input must be an integer\")\n",
    "\n",
    "  # Handle special cases for numbers ending in 11, 12, and 13\n",
    "  if num % 100 in [11, 12, 13]:\n",
    "    return str(num) + \"th\"\n",
    "\n",
    "  # Get the last digit of the number\n",
    "  last_digit = num % 10\n",
    "\n",
    "  # Choose the appropriate suffix based on the last digit\n",
    "  suffix = \"st\" if last_digit == 1 else (\"nd\" if last_digit == 2 else (\"rd\" if last_digit == 3 else \"th\"))\n",
    "\n",
    "  return str(num) + suffix\n",
    "\n",
    "def download_geopackage(url, filename=\"temp.gpkg\"):\n",
    "  \"\"\"\n",
    "  Downloads a geopackage file from a URL.\n",
    "\n",
    "  Args:\n",
    "      url: The URL of the online geopackage file.\n",
    "      filename: Temporary filename to store the downloaded geopackage (optional).\n",
    "\n",
    "  Returns:\n",
    "      The filename of the downloaded geopackage.\n",
    "  \"\"\"\n",
    "  # Download the geopackage file\n",
    "  response = requests.get(url, stream=True)\n",
    "  if response.status_code == 200:\n",
    "    with open(filename, 'wb') as f:\n",
    "      for chunk in response.iter_content(1024):\n",
    "        f.write(chunk)\n",
    "    print(f\"Succeeded in downloading geopackage from {url}\")\n",
    "    del response\n",
    "    return filename\n",
    "  else:\n",
    "    raise ValueError(f\"Failed to download geopackage from {url}\")\n",
    "\n",
    "\n",
    "# Maybe the problem is with the parameter shape_esm, try .shp file instead of .gpkg file\n",
    "\n",
    "def clip_data(np_data, shape_esm):\n",
    "    \"\"\"\n",
    "    Clips the DataArray 'data' using the geometry from 'shape_esm'.\n",
    "\n",
    "    Args:\n",
    "        np_data (xr.DataArray): The DataArray to clip.\n",
    "        shape_esm (geopandas.GeoDataFrame): GeoDataFrame containing the geometry to clip with.\n",
    "\n",
    "    Returns:\n",
    "        xr.DataArray: The clipped DataArray.\n",
    "    \"\"\"\n",
    "\n",
    "    lat = np.arange(60, -60, -0.04)  # 3000 rows\n",
    "    lon = np.arange(-180, 180, 0.04)  # 9000 cols\n",
    "\n",
    "    data = xr.DataArray(data=np_data, dims=[\"lat\", \"lon\"], coords=[lat, lon])\n",
    "    data.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    data.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "    data_esm = data.rio.clip(shape_esm.geometry.apply(mapping), shape_esm.crs, all_touched=True)\n",
    "\n",
    "    return data_esm \n",
    "\n",
    "def clip_array_with_geopackage(data, filename, crs=None):\n",
    "  \"\"\"\n",
    "  Clips a NumPy array using a geopackage file.\n",
    "\n",
    "  Args:\n",
    "      data: The NumPy array to clip.\n",
    "      filename: Path to the geopackage file.\n",
    "      crs: Coordinate Reference System (CRS) of the data and shapefile (optional).\n",
    "\n",
    "  Returns:\n",
    "      A NumPy array representing the clipped data.\n",
    "  \"\"\"\n",
    "\n",
    "  # Read the shapefile\n",
    "  gdf = gpd.read_file(filename)\n",
    "\n",
    "  # Get the first geometry (assuming only one shape is needed)\n",
    "  geometry = mapping(gdf.iloc[0].geometry)\n",
    "\n",
    "  # Open the raster dataset from the NumPy array\n",
    "  with rasterio.open(None, \"w\", driver=\"GTiff\", height=data.shape[0], width=data.shape[1], count=1, dtype=data.dtype) as src:\n",
    "    src.transform = rasterio.Affine.identity  # Assuming unit transform for simplicity\n",
    "    src.crs = crs if crs else CRS.from_epsg(4326)  # Default to EPSG:4326 (WGS84)\n",
    "    src.write(data, 1)\n",
    "\n",
    "    # Clip the raster by the geometry\n",
    "    clipped, transform = rasterio.rasterize(shapes=[geometry], out_shape=data.shape, fill=0, transform=src.transform, crs=src.crs, resampling=Resampling.nearest)\n",
    "\n",
    "  # Cleanup (optional, remove downloaded file after use)\n",
    "  # import os\n",
    "  # if os.path.exists(filename):\n",
    "  #   os.remove(filename)\n",
    "\n",
    "  return clipped\n",
    "\n",
    "def read_persiann_ccs(file_path):\n",
    "    ncols, nrows = 9000, 3000\n",
    "    data = np.zeros((nrows, ncols), dtype=np.float32)  # Initialize data array\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        for i in range(nrows):\n",
    "            for j in range(ncols):\n",
    "                # Read two bytes from the file, big-endian format\n",
    "                val = struct.unpack('>h', f.read(2))[0]\n",
    "                # Convert to mm/3hr, handling the no-data value\n",
    "                data[i, j] = np.nan if val == -9999 else val / 100.0\n",
    "\n",
    "    return data\n",
    "# return a numpy array\n",
    "def convert_to_geotiff(data, geotiff_path):\n",
    "    transform = from_origin(-180, 60, 0.04, 0.04)\n",
    "    metadata = {\n",
    "        'driver': 'GTiff',\n",
    "        'height': data.shape[0],\n",
    "        'width': data.shape[1],\n",
    "        'count': 1,\n",
    "        'dtype': 'float32',\n",
    "        'crs': '+proj=latlong',\n",
    "        'transform': transform\n",
    "    }\n",
    "    \n",
    "    with rasterio.open(geotiff_path, 'w', **metadata) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def clip_raster_with_gpkg(raster_path, gpkg_path, clipped_raster_path):\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "    gdf = gdf.to_crs(crs='+proj=latlong')\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        out_image, out_transform = mask(src, gdf.geometry, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        \n",
    "        out_meta.update({\n",
    "            'driver': 'GTiff',\n",
    "            'height': out_image.shape[1],\n",
    "            'width': out_image.shape[2],\n",
    "            'transform': out_transform\n",
    "        })\n",
    "        \n",
    "        with rasterio.open(clipped_raster_path, 'w', **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "# def plot_clipped_data(clipped_raster_path):\n",
    "#     with rasterio.open(clipped_raster_path) as src:\n",
    "#         data = src.read(1)\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.imshow(data, cmap='viridis', origin='upper')\n",
    "#         plt.colorbar(label='Precipitation (mm/3hr)')\n",
    "#         plt.title('Clipped PERSIANN-CCS Precipitation')\n",
    "#         plt.xlabel('Longitude')\n",
    "#         plt.ylabel('Latitude')\n",
    "#         plt.show()\n",
    "\n",
    "def plot_clipped_data(clipped_raster_array):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(clipped_raster_array, cmap='viridis', origin='upper')\n",
    "    plt.colorbar(label='Precipitation (mm/3hr)')\n",
    "    plt.title('Clipped PERSIANN-CCS Precipitation')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()\n",
    "\n",
    "# # Paths\n",
    "# file_path = \"C:/Users/liang.yang/Downloads/rgccs3h0300100.bin.gz\"\n",
    "# geotiff_path = '/home/hidrologia/Downloads/rgccs3h0300100.tif'\n",
    "# gpkg_path = '/home/hidrologia/Pronostico_hidro/Inputs/Manduriacu_delimitada.gpkg'\n",
    "# clipped_raster_path = '/home/hidrologia/Downloads/clipped_rgccs3h0300100.tif'\n",
    "\n",
    "# # Read and convert the data to GeoTIFF\n",
    "# data = read_persiann_ccs(file_path)\n",
    "# convert_to_geotiff(data, geotiff_path)\n",
    "\n",
    "# # Clip the GeoTIFF with the geopackage shapefile\n",
    "# clip_raster_with_gpkg(geotiff_path, gpkg_path, clipped_raster_path)\n",
    "\n",
    "# # Plot the clipped data\n",
    "# plot_clipped_data(clipped_raster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded in downloading geopackage from https://github.com/DPAINAMHI/esmeraldas/raw/bba2fdfc0fa2098b99d9e106f4775cc42b56de8a/Basins/esmeraldas.gpkg\n",
      "8 datasets from 2024-03-01 to 2024-03-01 with a frequency of 3h will be downloaded from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/\n",
      "\n",
      "***********************************************************\n",
      "Start to process the 1st slice, 1 slices left\n",
      "\n",
      "Start to download the 1st file of the 1st slice from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406100.bin.gz, 3 files left in this slice\n",
      "Data successfully downloaded from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406100.bin.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leon/opt/anaconda3/envs/Py_3_10/lib/python3.10/site-packages/rasterio/io.py:141: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  return writer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406100.bin.gz: Input shapes do not overlap raster.\n",
      "Start to download the 2nd file of the 1st slice from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406103.bin.gz, 2 files left in this slice\n",
      "Data successfully downloaded from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406103.bin.gz\n",
      "Error processing URL https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406103.bin.gz: Input shapes do not overlap raster.\n",
      "Start to download the 3rd file of the 1st slice from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406106.bin.gz, 1 files left in this slice\n",
      "Data successfully downloaded from https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406106.bin.gz\n",
      "Error processing URL https://persiann.eng.uci.edu/CHRSdata/PERSIANN-CCS/3hrly/rgccs3h2406106.bin.gz: Input shapes do not overlap raster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrow = 3000\n",
    "ncol = 9000 # As per the readme file of the raw dataset\n",
    "start_year=2024\n",
    "start_month=3\n",
    "start_day=1\n",
    "end_year=2024\n",
    "end_month=3\n",
    "end_day=1\n",
    "interval=3 # choose from 3 and 6\n",
    "shape_file_url = \"https://github.com/DPAINAMHI/esmeraldas/raw/bba2fdfc0fa2098b99d9e106f4775cc42b56de8a/Basins/esmeraldas.gpkg\"\n",
    "max_obs_per_slice=4\n",
    "# Download the shape file of Esmeraldas from the github repo\n",
    "shape_file = download_geopackage(shape_file_url)\n",
    "shape_file=gpd.read_file(shape_file)\n",
    "\n",
    "# Get the list of URLs and desired time range of data\n",
    "slice_list, num_of_obs = iter_url(start_year,start_month,start_day,end_year,end_month,end_day,interval,max_obs_per_slice)\n",
    "t_downloaded=0\n",
    "for t, slice in enumerate(slice_list):\n",
    "\n",
    "    print(\"\\n***********************************************************\")\n",
    "    print(f\"Start to process the {num_to_ordinal(t+1)} slice, {len(slice_list)-t-1} slices left\\n\")\n",
    "    \n",
    "    time_range = xr.DataArray(dims='time',data=slice['date_slice'])\n",
    "    \n",
    "    array_list=[] # Null container of all the clipped array\n",
    "    for i, url in enumerate(slice['urls']):\n",
    "        try:\n",
    "            print(f\"Start to download the {num_to_ordinal(i+1)} file of the {num_to_ordinal(t+1)} slice from {url}, {max_obs_per_slice-i-1} files left in this slice\")\n",
    "            array_esm_temp = clip_data(read_persiann_css_online0(url, nrow, ncol), shape_file)\n",
    "            t_downloaded+=1\n",
    "            array_list.append(array_esm_temp)\n",
    "            print(f\"Succeeded in storing data of {time_range[i].data} \\n{t_downloaded} of {num_of_obs} downloaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")    \n",
    "            time_range = time_range.drop_isel({\"time\":i}) \n",
    "    dataset = xr.concat(array_list, dim=time_range.variable)\n",
    "    dataset = dataset.drop_vars('spatial_ref')\n",
    "    data_dict = {\"precipitation\": dataset}\n",
    "    ds = xr.Dataset(data_dict)\n",
    "    ds = ds.astype(np.float32)\n",
    "    slice_start = str(slice['start_datetime'].date())+\"HH\"+format_number_with_zeros(slice['start_datetime'].hour,2)\n",
    "    slice_start = slice_start.replace('-','')\n",
    "    slice_end = str(slice['end_datetime'].date())+\"HH\"+format_number_with_zeros(slice['end_datetime'].hour,2)\n",
    "    slice_end = slice_end.replace('-','')\n",
    "    dest_folder_name = \"clipped_ds_\"+str(interval)+'h'\n",
    "    # Check if the new folder exists\n",
    "    if not os.path.exists(dest_folder_name):\n",
    "        # Create the new folder if it already exists in the current work dir\n",
    "        os.makedirs(dest_folder_name)\n",
    "    # Define the filename for the dataset\n",
    "    output_filename = slice_start + '__' + slice_end +'.nc'\n",
    "    # Combine the folder path and filename\n",
    "    full_path = os.path.join(dest_folder_name, output_filename)\n",
    "    ds.to_netcdf(full_path)\n",
    "    print(f\"Clipped arrays from {slice_start} to {slice_end} saved to {full_path}\")\n",
    "    print(f\"This is the {num_to_ordinal(t+1)} file of {len(slice_list)} in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the clipped data between the ouput of this script and the original one from the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with the case of 2024-03-01 03:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df_ccs.loc['2024-03-01 12:00:00']\n",
    "a = a.replace(-99,np.nan)\n",
    "a = np.array(a)\n",
    "a = a.reshape(53,52)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ds.sel(time='2024-03-01T12:00:00.000000000')['precipitation'].data\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the number of non-nan pixels is different from the standard one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.reshape(b,-1)\n",
    "b = b[~np.isnan(b)]\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafts Below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
